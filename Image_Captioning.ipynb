{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP8ieXMBN78OkwqCWihRn6E",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Karn2898/Visual-semantic-pipeline/blob/main/Image_Captioning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fdfm-veok-sC",
        "outputId": "724e07b4-d1b9-4329-ece8-d66a725711b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cpu)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.24.0+cpu)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.12/dist-packages (3.8.11)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (11.3.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.15)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.13)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (8.3.10)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.5.2)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.4.3)\n",
            "Requirement already satisfied: typer-slim<1.0.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.20.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.32.4)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.12.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.11.12)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.3)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer-slim<1.0.0,>=0.3.0->spacy) (8.3.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.4.2->spacy) (0.23.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.4.2->spacy) (7.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.4.2->spacy) (2.0.1)\n"
          ]
        }
      ],
      "source": [
        "pip install torch torchvision spacy pillow\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cd428dd2",
        "outputId": "c05f45c8-a28e-4bdb-e861-63b445ce402f"
      },
      "source": [
        "!python -m spacy download en_core_web_sm"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m71.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/Flickr8k_text.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hsFVAUmtvaty",
        "outputId": "3acf31f8-a326-4d54-c10a-18e77fa21b94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/Flickr8k_text.zip\n",
            "  inflating: CrowdFlowerAnnotations.txt  \n",
            "  inflating: ExpertAnnotations.txt   \n",
            "  inflating: Flickr8k.lemma.token.txt  \n",
            "   creating: __MACOSX/\n",
            "  inflating: __MACOSX/._Flickr8k.lemma.token.txt  \n",
            "  inflating: Flickr8k.token.txt      \n",
            "  inflating: Flickr_8k.devImages.txt  \n",
            "  inflating: Flickr_8k.testImages.txt  \n",
            "  inflating: Flickr_8k.trainImages.txt  \n",
            "  inflating: readme.txt              \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def format_real_flickr_data():\n",
        "    input_file = \"Flickr8k.token.txt\"\n",
        "    output_file = \"captions.txt\"\n",
        "\n",
        "    imgs = []\n",
        "    caps = []\n",
        "\n",
        "    print(\"Reading real Flickr8k data...\")\n",
        "    try:\n",
        "        with open(input_file, \"r\") as f:\n",
        "            for line in f:\n",
        "\n",
        "                tokens = line.split(\"\\t\")\n",
        "                if len(tokens) < 2:\n",
        "                    continue\n",
        "\n",
        "\n",
        "                img_id = tokens[0].split(\"#\")[0]\n",
        "                caption = tokens[1].strip()\n",
        "\n",
        "                imgs.append(img_id)\n",
        "                caps.append(caption)\n",
        "\n",
        "        # Create Dataframe and save\n",
        "        df = pd.DataFrame({\"image\": imgs, \"caption\": caps})\n",
        "        df.to_csv(output_file, index=False)\n",
        "        print(f\"Success! Saved {len(df)} captions to {output_file}.\")\n",
        "\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: Could not find {input_file}. Please download the dataset first.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    format_real_flickr_data()\n"
      ],
      "metadata": {
        "id": "5CW_blhXoBND",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59399e8b-f7b5-4a8d-94ab-590fc2050eea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading real Flickr8k data...\n",
            "Success! Saved 40460 captions to captions.txt.\n",
            "You can now run data_loader.py with the REAL dataset.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import spacy\n",
        "import torch\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import transforms\n",
        "from typing import List, Tuple, Any\n",
        "\n",
        "\n",
        "spacy_eng = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "class Vocabulary:\n",
        "\n",
        "    def __init__(self, freq_threshold: int):\n",
        "        self.itos = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\"}\n",
        "        self.stoi = {\"<PAD>\": 0, \"<SOS>\": 1, \"<EOS>\": 2, \"<UNK>\": 3}\n",
        "        self.freq_threshold = freq_threshold\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.itos)\n",
        "\n",
        "    @staticmethod\n",
        "    def tokenizer_eng(text: str) -> List[str]:\n",
        "        return [tok.text.lower() for tok in spacy_eng.tokenizer(text)]\n",
        "\n",
        "    def build_vocabulary(self, sentence_list: List[str]):\n",
        "        frequencies = {}\n",
        "        idx = 4\n",
        "\n",
        "        for sentence in sentence_list:\n",
        "            for word in self.tokenizer_eng(sentence):\n",
        "                if word not in frequencies:\n",
        "                    frequencies[word] = 1\n",
        "                else:\n",
        "                    frequencies[word] += 1\n",
        "\n",
        "                if frequencies[word] == self.freq_threshold:\n",
        "                    self.stoi[word] = idx\n",
        "                    self.itos[idx] = word\n",
        "                    idx += 1\n",
        "\n",
        "    def numericalize(self, text: str) -> List[int]:\n",
        "        tokenized_text = self.tokenizer_eng(text)\n",
        "        return [\n",
        "            self.stoi[token] if token in self.stoi else self.stoi[\"<UNK>\"]\n",
        "            for token in tokenized_text\n",
        "        ]\n",
        "\n",
        "class FlickrDataset(Dataset):\n",
        "\n",
        "    def __init__(self, root_dir: str, captions_file: str, transform: Any = None, freq_threshold: int = 5):\n",
        "        self.root_dir = root_dir\n",
        "        self.df = pd.read_csv(captions_file)\n",
        "        self.transform = transform\n",
        "\n",
        "        self.imgs = self.df[\"image\"]\n",
        "        self.captions = self.df[\"caption\"]\n",
        "\n",
        "\n",
        "        self.vocab = Vocabulary(freq_threshold)\n",
        "        self.vocab.build_vocabulary(self.captions.tolist())\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, index: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        caption = self.captions[index]\n",
        "        img_id = self.imgs[index]\n",
        "        img_path = os.path.join(self.root_dir, img_id)\n",
        "\n",
        "\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "\n",
        "\n",
        "        numericalized_caption = [self.vocab.stoi[\"<SOS>\"]]\n",
        "        numericalized_caption += self.vocab.numericalize(caption)\n",
        "        numericalized_caption += [self.vocab.stoi[\"<EOS>\"]]\n",
        "\n",
        "        return img, torch.tensor(numericalized_caption)\n",
        "\n",
        "class MyCollate:\n",
        "\n",
        "    def __init__(self, pad_idx: int):\n",
        "        self.pad_idx = pad_idx\n",
        "\n",
        "    def __call__(self, batch: List[Tuple[torch.Tensor, torch.Tensor]]) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        imgs = [item[0].unsqueeze(0) for item in batch]\n",
        "        imgs = torch.cat(imgs, dim=0)\n",
        "\n",
        "        targets = [item[1] for item in batch]\n",
        "\n",
        "        targets = pad_sequence(targets, batch_first=True, padding_value=self.pad_idx)\n",
        "\n",
        "        return imgs, targets\n",
        "\n",
        "def get_loader(\n",
        "    root_folder: str,\n",
        "    annotation_file: str,\n",
        "    transform: Any,\n",
        "    batch_size: int = 32,\n",
        "    num_workers: int = 8,\n",
        "    shuffle: bool = True,\n",
        "    pin_memory: bool = True,\n",
        ") -> Tuple[DataLoader, FlickrDataset]:\n",
        "\n",
        "    dataset = FlickrDataset(root_folder, annotation_file, transform=transform)\n",
        "    pad_idx = dataset.vocab.stoi[\"<PAD>\"]\n",
        "\n",
        "    loader = DataLoader(\n",
        "        dataset=dataset,\n",
        "        batch_size=batch_size,\n",
        "        num_workers=num_workers,\n",
        "        shuffle=shuffle,\n",
        "        pin_memory=pin_memory,\n",
        "        collate_fn=MyCollate(pad_idx=pad_idx),\n",
        "    )\n",
        "\n",
        "    return loader, dataset\n"
      ],
      "metadata": {
        "id": "oUdVWXpEvyzf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import math\n",
        "\n",
        "class EncoderCNN(nn.Module):\n",
        "\n",
        "    def __init__(self, embed_size, train_CNN=False):\n",
        "        super(EncoderCNN, self).__init__()\n",
        "        resnet = models.resnet101(weights=models.ResNet101_Weights.DEFAULT)\n",
        "\n",
        "        modules = list(resnet.children())[:-2]\n",
        "        self.resnet = nn.Sequential(*modules)\n",
        "\n",
        "        self.embed = nn.Linear(2048, embed_size)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.train_CNN = train_CNN\n",
        "\n",
        "    def forward(self, images):\n",
        "        with torch.set_grad_enabled(self.train_CNN):\n",
        "            features = self.resnet(images)\n",
        "\n",
        "\n",
        "        features = features.permute(0, 2, 3, 1)\n",
        "        features = features.view(features.size(0), -1, features.size(3))\n",
        "\n",
        "\n",
        "        features = self.embed(features)\n",
        "        features = self.relu(features)\n",
        "\n",
        "        return features\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1) # [Max_Len, 1, D_Model]\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:x.size(0), :]\n",
        "\n",
        "class DecoderTransformer(nn.Module):\n",
        "    def __init__(self, embed_size, vocab_size, num_heads=4, num_layers=2, max_len=100):\n",
        "        super(DecoderTransformer, self).__init__()\n",
        "\n",
        "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
        "        self.pos_encoder = PositionalEncoding(embed_size, max_len)\n",
        "\n",
        "\n",
        "        decoder_layer = nn.TransformerDecoderLayer(d_model=embed_size, nhead=num_heads)\n",
        "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
        "\n",
        "        self.linear = nn.Linear(embed_size, vocab_size)\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def forward(self, features, captions):\n",
        "\n",
        "        # Embed captions\n",
        "        embeddings = self.embed(captions) * math.sqrt(features.size(-1))\n",
        "\n",
        "\n",
        "        embeddings = embeddings.permute(1, 0, 2)\n",
        "        features = features.permute(1, 0, 2)\n",
        "\n",
        "        # positional encoding\n",
        "        embeddings = self.pos_encoder(embeddings)\n",
        "\n",
        "        tgt_mask = self.generate_square_subsequent_mask(embeddings.size(0)).to(features.device)\n",
        "\n",
        "        outputs = self.transformer_decoder(tgt=embeddings, memory=features, tgt_mask=tgt_mask)\n",
        "\n",
        "        outputs = self.linear(outputs)\n",
        "\n",
        "        return outputs.permute(1, 0, 2)\n",
        "\n",
        "    def generate_square_subsequent_mask(self, sz):\n",
        "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
        "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "        return mask\n",
        "\n",
        "    def caption_image(self, image_features, vocabulary, max_length=20):\n",
        "\n",
        "        batch_size = image_features.size(0)\n",
        "        start_token = vocabulary.stoi[\"<SOS>\"]\n",
        "\n",
        "        generated = torch.tensor([start_token]).unsqueeze(1).to(image_features.device)\n",
        "\n",
        "        image_features = image_features.permute(1, 0, 2)\n",
        "\n",
        "        result_caption = []\n",
        "\n",
        "        for _ in range(max_length):\n",
        "\n",
        "            tgt_emb = self.embed(generated) * math.sqrt(image_features.size(-1))\n",
        "            tgt_emb = tgt_emb.permute(1, 0, 2)\n",
        "            tgt_emb = self.pos_encoder(tgt_emb)\n",
        "\n",
        "            # Mask\n",
        "            mask = self.generate_square_subsequent_mask(tgt_emb.size(0)).to(image_features.device)\n",
        "\n",
        "            # Decode\n",
        "            out = self.transformer_decoder(tgt=tgt_emb, memory=image_features, tgt_mask=mask)\n",
        "\n",
        "            # Get last token output\n",
        "            last_token_out = out[-1, :, :]\n",
        "\n",
        "            # Predict\n",
        "            logits = self.linear(last_token_out)\n",
        "            predicted_id = logits.argmax(1).item()\n",
        "\n",
        "            result_caption.append(vocabulary.itos[predicted_id])\n",
        "\n",
        "            if vocabulary.itos[predicted_id] == \"<EOS>\":\n",
        "                break\n",
        "\n",
        "\n",
        "            next_token = torch.tensor([predicted_id]).unsqueeze(1).to(image_features.device)\n",
        "            generated = torch.cat((generated, next_token), dim=1)\n",
        "\n",
        "        return result_caption\n",
        "\n",
        "class ImageCaptionModel(nn.Module):\n",
        "    def __init__(self, embed_size, hidden_size, vocab_size, num_heads, num_layers, train_CNN=False):\n",
        "        super(ImageCaptionModel, self).__init__()\n",
        "        self.encoder = EncoderCNN(embed_size, train_CNN)\n",
        "        self.decoder = DecoderTransformer(embed_size, vocab_size, num_heads, num_layers)\n",
        "\n",
        "    def forward(self, images, captions):\n",
        "        features = self.encoder(images)\n",
        "        outputs = self.decoder(features, captions)\n",
        "        return outputs\n",
        "\n",
        "    def caption_image(self, image, vocabulary, max_length=20):\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            features = self.encoder(image.unsqueeze(0))\n",
        "            return self.decoder.caption_image(features, vocabulary, max_length)\n",
        "\n",
        "# Test\n",
        "if __name__ == \"__main__\":\n",
        "    embed_size = 256\n",
        "    vocab_size = 1000\n",
        "    model = ImageCaptionModel(embed_size, 256, vocab_size, num_heads=4, num_layers=2)\n",
        "\n",
        "    img = torch.randn(2, 3, 224, 224)\n",
        "    caps = torch.randint(0, 1000, (2, 20))\n",
        "\n",
        "    out = model(img, caps)\n",
        "    print(f\"Transformer Output Shape: {out.shape}\")\n"
      ],
      "metadata": {
        "id": "9VrmR7T3QuGk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}