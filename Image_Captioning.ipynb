{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOEwrZaJ2ahVuoR9FJfjOnN"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fdfm-veok-sC",
        "outputId": "724e07b4-d1b9-4329-ece8-d66a725711b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cpu)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.24.0+cpu)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.12/dist-packages (3.8.11)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (11.3.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.15)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.13)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (8.3.10)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.5.2)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.4.3)\n",
            "Requirement already satisfied: typer-slim<1.0.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.20.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.32.4)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.12.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.11.12)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.3)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer-slim<1.0.0,>=0.3.0->spacy) (8.3.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.4.2->spacy) (0.23.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.4.2->spacy) (7.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.4.2->spacy) (2.0.1)\n"
          ]
        }
      ],
      "source": [
        "pip install torch torchvision spacy pillow\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cd428dd2",
        "outputId": "c05f45c8-a28e-4bdb-e861-63b445ce402f"
      },
      "source": [
        "!python -m spacy download en_core_web_sm"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m71.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/Flickr8k_text.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hsFVAUmtvaty",
        "outputId": "98440e47-32e2-409a-fd8a-07324fdb313b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/Flickr8k_text.zip\n",
            "  inflating: CrowdFlowerAnnotations.txt  \n",
            "  inflating: ExpertAnnotations.txt   \n",
            "  inflating: Flickr8k.lemma.token.txt  \n",
            "   creating: __MACOSX/\n",
            "  inflating: __MACOSX/._Flickr8k.lemma.token.txt  \n",
            "  inflating: Flickr8k.token.txt      \n",
            "  inflating: Flickr_8k.devImages.txt  \n",
            "  inflating: Flickr_8k.testImages.txt  \n",
            "  inflating: Flickr_8k.trainImages.txt  \n",
            "  inflating: readme.txt              \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def format_real_flickr_data():\n",
        "    input_file = \"Flickr8k.token.txt\"\n",
        "    output_file = \"captions.txt\"\n",
        "\n",
        "    imgs = []\n",
        "    caps = []\n",
        "\n",
        "    print(\"Reading real Flickr8k data...\")\n",
        "    try:\n",
        "        with open(input_file, \"r\") as f:\n",
        "            for line in f:\n",
        "\n",
        "                tokens = line.split(\"\\t\")\n",
        "                if len(tokens) < 2:\n",
        "                    continue\n",
        "\n",
        "\n",
        "                img_id = tokens[0].split(\"#\")[0]\n",
        "                caption = tokens[1].strip()\n",
        "\n",
        "                imgs.append(img_id)\n",
        "                caps.append(caption)\n",
        "\n",
        "        # Create Dataframe and save\n",
        "        df = pd.DataFrame({\"image\": imgs, \"caption\": caps})\n",
        "        df.to_csv(output_file, index=False)\n",
        "        print(f\"Success! Saved {len(df)} captions to {output_file}.\")\n",
        "\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: Could not find {input_file}. Please download the dataset first.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    format_real_flickr_data()\n"
      ],
      "metadata": {
        "id": "5CW_blhXoBND",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8bba9445-e427-4e41-e6b6-93af373f2b54"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading real Flickr8k data...\n",
            "Success! Saved 40460 captions to captions.txt.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import spacy\n",
        "import torch\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import transforms\n",
        "from typing import List, Tuple, Any\n",
        "\n",
        "\n",
        "spacy_eng = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "class Vocabulary:\n",
        "\n",
        "    def __init__(self, freq_threshold: int):\n",
        "        self.itos = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\"}\n",
        "        self.stoi = {\"<PAD>\": 0, \"<SOS>\": 1, \"<EOS>\": 2, \"<UNK>\": 3}\n",
        "        self.freq_threshold = freq_threshold\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.itos)\n",
        "\n",
        "    @staticmethod\n",
        "    def tokenizer_eng(text: str) -> List[str]:\n",
        "        return [tok.text.lower() for tok in spacy_eng.tokenizer(text)]\n",
        "\n",
        "    def build_vocabulary(self, sentence_list: List[str]):\n",
        "        frequencies = {}\n",
        "        idx = 4\n",
        "\n",
        "        for sentence in sentence_list:\n",
        "            for word in self.tokenizer_eng(sentence):\n",
        "                if word not in frequencies:\n",
        "                    frequencies[word] = 1\n",
        "                else:\n",
        "                    frequencies[word] += 1\n",
        "\n",
        "                if frequencies[word] == self.freq_threshold:\n",
        "                    self.stoi[word] = idx\n",
        "                    self.itos[idx] = word\n",
        "                    idx += 1\n",
        "\n",
        "    def numericalize(self, text: str) -> List[int]:\n",
        "        tokenized_text = self.tokenizer_eng(text)\n",
        "        return [\n",
        "            self.stoi[token] if token in self.stoi else self.stoi[\"<UNK>\"]\n",
        "            for token in tokenized_text\n",
        "        ]\n",
        "\n",
        "class FlickrDataset(Dataset):\n",
        "\n",
        "    def __init__(self, root_dir: str, captions_file: str, transform: Any = None, freq_threshold: int = 5):\n",
        "        self.root_dir = root_dir\n",
        "        self.df = pd.read_csv(captions_file)\n",
        "        self.transform = transform\n",
        "\n",
        "        self.imgs = self.df[\"image\"]\n",
        "        self.captions = self.df[\"caption\"]\n",
        "\n",
        "\n",
        "        self.vocab = Vocabulary(freq_threshold)\n",
        "        self.vocab.build_vocabulary(self.captions.tolist())\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, index: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        caption = self.captions[index]\n",
        "        img_id = self.imgs[index]\n",
        "        img_path = os.path.join(self.root_dir, img_id)\n",
        "\n",
        "\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "\n",
        "\n",
        "        numericalized_caption = [self.vocab.stoi[\"<SOS>\"]]\n",
        "        numericalized_caption += self.vocab.numericalize(caption)\n",
        "        numericalized_caption += [self.vocab.stoi[\"<EOS>\"]]\n",
        "\n",
        "        return img, torch.tensor(numericalized_caption)\n",
        "\n",
        "class MyCollate:\n",
        "\n",
        "    def __init__(self, pad_idx: int):\n",
        "        self.pad_idx = pad_idx\n",
        "\n",
        "    def __call__(self, batch: List[Tuple[torch.Tensor, torch.Tensor]]) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        imgs = [item[0].unsqueeze(0) for item in batch]\n",
        "        imgs = torch.cat(imgs, dim=0)\n",
        "\n",
        "        targets = [item[1] for item in batch]\n",
        "\n",
        "        targets = pad_sequence(targets, batch_first=True, padding_value=self.pad_idx)\n",
        "\n",
        "        return imgs, targets\n",
        "\n",
        "def get_loader(\n",
        "    root_folder: str,\n",
        "    annotation_file: str,\n",
        "    transform: Any,\n",
        "    batch_size: int = 32,\n",
        "    num_workers: int = 8,\n",
        "    shuffle: bool = True,\n",
        "    pin_memory: bool = True,\n",
        ") -> Tuple[DataLoader, FlickrDataset]:\n",
        "\n",
        "    dataset = FlickrDataset(root_folder, annotation_file, transform=transform)\n",
        "    pad_idx = dataset.vocab.stoi[\"<PAD>\"]\n",
        "\n",
        "    loader = DataLoader(\n",
        "        dataset=dataset,\n",
        "        batch_size=batch_size,\n",
        "        num_workers=num_workers,\n",
        "        shuffle=shuffle,\n",
        "        pin_memory=pin_memory,\n",
        "        collate_fn=MyCollate(pad_idx=pad_idx),\n",
        "    )\n",
        "\n",
        "    return loader, dataset\n"
      ],
      "metadata": {
        "id": "oUdVWXpEvyzf"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import math\n",
        "\n",
        "class EncoderCNN(nn.Module):\n",
        "\n",
        "    def __init__(self, embed_size, train_CNN=False):\n",
        "        super(EncoderCNN, self).__init__()\n",
        "        resnet = models.resnet101(weights=models.ResNet101_Weights.DEFAULT)\n",
        "\n",
        "        modules = list(resnet.children())[:-2]\n",
        "        self.resnet = nn.Sequential(*modules)\n",
        "\n",
        "        self.embed = nn.Linear(2048, embed_size)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.train_CNN = train_CNN\n",
        "\n",
        "    def forward(self, images):\n",
        "        with torch.set_grad_enabled(self.train_CNN):\n",
        "            features = self.resnet(images)\n",
        "\n",
        "\n",
        "        features = features.permute(0, 2, 3, 1)\n",
        "        features = features.view(features.size(0), -1, features.size(3))\n",
        "\n",
        "\n",
        "        features = self.embed(features)\n",
        "        features = self.relu(features)\n",
        "\n",
        "        return features\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1) # [Max_Len, 1, D_Model]\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:x.size(0), :]\n",
        "\n",
        "class DecoderTransformer(nn.Module):\n",
        "    def __init__(self, embed_size, vocab_size, num_heads=4, num_layers=2, max_len=100):\n",
        "        super(DecoderTransformer, self).__init__()\n",
        "\n",
        "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
        "        self.pos_encoder = PositionalEncoding(embed_size, max_len)\n",
        "\n",
        "\n",
        "        decoder_layer = nn.TransformerDecoderLayer(d_model=embed_size, nhead=num_heads)\n",
        "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
        "\n",
        "        self.linear = nn.Linear(embed_size, vocab_size)\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def forward(self, features, captions):\n",
        "\n",
        "        # Embed captions\n",
        "        embeddings = self.embed(captions) * math.sqrt(features.size(-1))\n",
        "\n",
        "\n",
        "        embeddings = embeddings.permute(1, 0, 2)\n",
        "        features = features.permute(1, 0, 2)\n",
        "\n",
        "        # positional encoding\n",
        "        embeddings = self.pos_encoder(embeddings)\n",
        "\n",
        "        tgt_mask = self.generate_square_subsequent_mask(embeddings.size(0)).to(features.device)\n",
        "\n",
        "        outputs = self.transformer_decoder(tgt=embeddings, memory=features, tgt_mask=tgt_mask)\n",
        "\n",
        "        outputs = self.linear(outputs)\n",
        "\n",
        "        return outputs.permute(1, 0, 2)\n",
        "\n",
        "    def generate_square_subsequent_mask(self, sz):\n",
        "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
        "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "        return mask\n",
        "\n",
        "    def caption_image(self, image_features, vocabulary, max_length=20):\n",
        "\n",
        "        batch_size = image_features.size(0)\n",
        "        start_token = vocabulary.stoi[\"<SOS>\"]\n",
        "\n",
        "        generated = torch.tensor([start_token]).unsqueeze(1).to(image_features.device)\n",
        "\n",
        "        image_features = image_features.permute(1, 0, 2)\n",
        "\n",
        "        result_caption = []\n",
        "\n",
        "        for _ in range(max_length):\n",
        "\n",
        "            tgt_emb = self.embed(generated) * math.sqrt(image_features.size(-1))\n",
        "            tgt_emb = tgt_emb.permute(1, 0, 2)\n",
        "            tgt_emb = self.pos_encoder(tgt_emb)\n",
        "\n",
        "            # Mask\n",
        "            mask = self.generate_square_subsequent_mask(tgt_emb.size(0)).to(image_features.device)\n",
        "\n",
        "            # Decode\n",
        "            out = self.transformer_decoder(tgt=tgt_emb, memory=image_features, tgt_mask=mask)\n",
        "\n",
        "            # Get last token output\n",
        "            last_token_out = out[-1, :, :]\n",
        "\n",
        "            # Predict\n",
        "            logits = self.linear(last_token_out)\n",
        "            predicted_id = logits.argmax(1).item()\n",
        "\n",
        "            result_caption.append(vocabulary.itos[predicted_id])\n",
        "\n",
        "            if vocabulary.itos[predicted_id] == \"<EOS>\":\n",
        "                break\n",
        "\n",
        "\n",
        "            next_token = torch.tensor([predicted_id]).unsqueeze(1).to(image_features.device)\n",
        "            generated = torch.cat((generated, next_token), dim=1)\n",
        "\n",
        "        return result_caption\n",
        "\n",
        "class ImageCaptionModel(nn.Module):\n",
        "    def __init__(self, embed_size, hidden_size, vocab_size, num_heads, num_layers, train_CNN=False):\n",
        "        super(ImageCaptionModel, self).__init__()\n",
        "        self.encoder = EncoderCNN(embed_size, train_CNN)\n",
        "        self.decoder = DecoderTransformer(embed_size, vocab_size, num_heads, num_layers)\n",
        "\n",
        "    def forward(self, images, captions):\n",
        "        features = self.encoder(images)\n",
        "        outputs = self.decoder(features, captions)\n",
        "        return outputs\n",
        "\n",
        "    def caption_image(self, image, vocabulary, max_length=20):\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            features = self.encoder(image.unsqueeze(0))\n",
        "            return self.decoder.caption_image(features, vocabulary, max_length)\n",
        "\n",
        "# Test\n",
        "if __name__ == \"__main__\":\n",
        "    embed_size = 256\n",
        "    vocab_size = 1000\n",
        "    model = ImageCaptionModel(embed_size, 256, vocab_size, num_heads=4, num_layers=2)\n",
        "\n",
        "    img = torch.randn(2, 3, 224, 224)\n",
        "    caps = torch.randint(0, 1000, (2, 20))\n",
        "\n",
        "    out = model(img, caps)\n",
        "    print(f\"Transformer Output Shape: {out.shape}\")\n"
      ],
      "metadata": {
        "id": "9VrmR7T3QuGk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7687f76-9c8d-4ecd-e092-1dd0aea6561d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet101-cd907fc2.pth\" to /root/.cache/torch/hub/checkpoints/resnet101-cd907fc2.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 171M/171M [00:00<00:00, 201MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transformer Output Shape: torch.Size([2, 20, 1000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "\n",
        "def print_examples(model, device, dataset):\n",
        "    \"\"\"\n",
        "    Helper to print predicted captions for a few images during training\n",
        "    to manually verify progress.\n",
        "    \"\"\"\n",
        "    transform = transforms.Compose(\n",
        "        [\n",
        "            transforms.Resize((224, 224)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "\n",
        "    test_img_dir = \"images/\"\n",
        "\n",
        "    import os\n",
        "    try:\n",
        "        test_images = [f for f in os.listdir(test_img_dir) if f.endswith('.jpg')][:2]\n",
        "    except FileNotFoundError:\n",
        "        print(\"Image directory not found, skipping examples.\")\n",
        "        return\n",
        "\n",
        "    for img_name in test_images:\n",
        "        image = Image.open(os.path.join(test_img_dir, img_name)).convert(\"RGB\")\n",
        "        image_tensor = transform(image).unsqueeze(0).to(device)\n",
        "\n",
        "        # Generate caption\n",
        "        with torch.no_grad():\n",
        "            caption = model.caption_image(image_tensor.squeeze(0), dataset.vocab)\n",
        "\n",
        "        print(f\"Image: {img_name}\")\n",
        "        print(f\"Prediction: {' '.join(caption)}\")\n",
        "        print(\"-\" * 20)\n",
        "\n",
        "    model.train()\n",
        "\n",
        "def save_checkpoint(state, filename=\"my_checkpoint.pth.tar\"):\n",
        "    print(\"=> Saving checkpoint\")\n",
        "    torch.save(state, filename)\n",
        "\n",
        "def load_checkpoint(checkpoint, model, optimizer):\n",
        "    print(\"=> Loading checkpoint\")\n",
        "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
        "    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n"
      ],
      "metadata": {
        "id": "jVzPtYPg6ICl"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "from data_loader import get_loader\n",
        "from model import ImageCaptionModel\n",
        "from utils import save_checkpoint, load_checkpoint, print_examples\n",
        "\n",
        "#  Hyperparameters\n",
        "embed_size = 256\n",
        "hidden_size = 256\n",
        "vocab_size = -1\n",
        "num_heads = 4\n",
        "num_layers = 2\n",
        "learning_rate = 3e-4\n",
        "num_epochs = 100\n",
        "batch_size = 32\n",
        "num_workers = 2\n",
        "load_model = False\n",
        "save_model = True\n",
        "train_CNN = False\n",
        "def train():\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "\n",
        "    transform = transforms.Compose(\n",
        "        [\n",
        "            transforms.Resize((232, 232)),\n",
        "            transforms.RandomCrop((224, 224)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "\n",
        "    print(\"Loading Data...\")\n",
        "    loader, dataset = get_loader(\n",
        "        root_folder=\"images/\",\n",
        "        annotation_file=\"captions.txt\",\n",
        "        transform=transform,\n",
        "        batch_size=batch_size,\n",
        "        num_workers=num_workers,\n",
        "    )\n",
        "\n",
        "    vocab_size = len(dataset.vocab)\n",
        "    print(f\"Vocabulary Size: {vocab_size}\")\n",
        "\n",
        "    model = ImageCaptionModel(\n",
        "        embed_size=embed_size,\n",
        "        hidden_size=hidden_size,\n",
        "        vocab_size=vocab_size,\n",
        "        num_heads=num_heads,\n",
        "        num_layers=num_layers,\n",
        "        train_CNN=train_CNN\n",
        "    ).to(device)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=dataset.vocab.stoi[\"<PAD>\"])\n",
        "\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    if load_model:\n",
        "        load_checkpoint(torch.load(\"my_checkpoint.pth.tar\"), model, optimizer)\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"\\n--- Epoch {epoch+1}/{num_epochs} ---\")\n",
        "\n",
        "        if save_model and epoch % 5 == 0:\n",
        "            checkpoint = {\n",
        "                \"state_dict\": model.state_dict(),\n",
        "                \"optimizer\": optimizer.state_dict(),\n",
        "            }\n",
        "            save_checkpoint(checkpoint)\n",
        "\n",
        "\n",
        "\n",
        "        loop = tqdm(loader, leave=True)\n",
        "\n",
        "        for idx, (imgs, captions) in enumerate(loop):\n",
        "            imgs = imgs.to(device)\n",
        "            captions = captions.to(device)\n",
        "\n",
        "\n",
        "            outputs = model(imgs, captions[:, :-1])\n",
        "\n",
        "\n",
        "            loss = criterion(\n",
        "                outputs.reshape(-1, outputs.shape[2]),\n",
        "                captions[:, 1:].reshape(-1)\n",
        "            )\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "\n",
        "\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            loop.set_description(f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
        "            loop.set_postfix(loss=loss.item())\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "id": "FHlswN3E6d3R",
        "outputId": "c6b8b91b-b824-472f-ef33-1fc723ddbaaa"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'data_loader'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1287003332.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdata_loader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_loader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImageCaptionModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msave_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_examples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'data_loader'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bf26bb7"
      },
      "source": [
        "# Task\n",
        "Refactor the existing image captioning notebook by organizing the `Vocabulary`, `FlickrDataset`, `MyCollate`, and `get_loader` into a data loader cell; `EncoderCNN`, `PositionalEncoding`, `DecoderTransformer`, and `ImageCaptionModel` into a model cell; and `print_examples`, `save_checkpoint`, and `load_checkpoint` into a utilities cell. Then, update the training cell by removing explicit imports for these components and execute all relevant cells."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0a0d0373"
      },
      "source": [
        "## Move Data Loader Code\n",
        "\n",
        "### Subtask:\n",
        "Create a new code cell above the training cell and move the `Vocabulary`, `FlickrDataset`, `MyCollate`, and `get_loader` definitions into it. This cell will effectively act as the 'data_loader'.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89908c66"
      },
      "source": [
        "**Reasoning**:\n",
        "To fulfill the subtask, I need to create a new code cell directly above the training cell (`FHlswN3E6d3R`) and populate it with the `Vocabulary`, `FlickrDataset`, `MyCollate`, and `get_loader` definitions from cell `oUdVWXpEvyzf`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09b3b34e"
      },
      "source": [
        "import os\n",
        "import spacy\n",
        "import torch\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import transforms\n",
        "from typing import List, Tuple, Any\n",
        "\n",
        "\n",
        "spacy_eng = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "class Vocabulary:\n",
        "\n",
        "    def __init__(self, freq_threshold: int):\n",
        "        self.itos = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\"}\n",
        "        self.stoi = {\"<PAD>\": 0, \"<SOS>\": 1, \"<EOS>\": 2, \"<UNK>\": 3}\n",
        "        self.freq_threshold = freq_threshold\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.itos)\n",
        "\n",
        "    @staticmethod\n",
        "    def tokenizer_eng(text: str) -> List[str]:\n",
        "        return [tok.text.lower() for tok in spacy_eng.tokenizer(text)]\n",
        "\n",
        "    def build_vocabulary(self, sentence_list: List[str]):\n",
        "        frequencies = {}\n",
        "        idx = 4\n",
        "\n",
        "        for sentence in sentence_list:\n",
        "            for word in self.tokenizer_eng(sentence):\n",
        "                if word not in frequencies:\n",
        "                    frequencies[word] = 1\n",
        "                else:\n",
        "                    frequencies[word] += 1\n",
        "\n",
        "                if frequencies[word] == self.freq_threshold:\n",
        "                    self.stoi[word] = idx\n",
        "                    self.itos[idx] = word\n",
        "                    idx += 1\n",
        "\n",
        "    def numericalize(self, text: str) -> List[int]:\n",
        "        tokenized_text = self.tokenizer_eng(text)\n",
        "        return [\n",
        "            self.stoi[token] if token in self.stoi else self.stoi[\"<UNK>\"]\n",
        "            for token in tokenized_text\n",
        "        ]\n",
        "\n",
        "class FlickrDataset(Dataset):\n",
        "\n",
        "    def __init__(self, root_dir: str, captions_file: str, transform: Any = None, freq_threshold: int = 5):\n",
        "        self.root_dir = root_dir\n",
        "        self.df = pd.read_csv(captions_file)\n",
        "        self.transform = transform\n",
        "\n",
        "        self.imgs = self.df[\"image\"]\n",
        "        self.captions = self.df[\"caption\"]\n",
        "\n",
        "\n",
        "        self.vocab = Vocabulary(freq_threshold)\n",
        "        self.vocab.build_vocabulary(self.captions.tolist())\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, index: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        caption = self.captions[index]\n",
        "        img_id = self.imgs[index]\n",
        "        img_path = os.path.join(self.root_dir, img_id)\n",
        "\n",
        "\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "\n",
        "\n",
        "        numericalized_caption = [self.vocab.stoi[\"<SOS>\"]]\n",
        "        numericalized_caption += self.vocab.numericalize(caption)\n",
        "        numericalized_caption += [self.vocab.stoi[\"<EOS>\"]]\n",
        "\n",
        "        return img, torch.tensor(numericalized_caption)\n",
        "\n",
        "class MyCollate:\n",
        "\n",
        "    def __init__(self, pad_idx: int):\n",
        "        self.pad_idx = pad_idx\n",
        "\n",
        "    def __call__(self, batch: List[Tuple[torch.Tensor, torch.Tensor]]) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        imgs = [item[0].unsqueeze(0) for item in batch]\n",
        "        imgs = torch.cat(imgs, dim=0)\n",
        "\n",
        "        targets = [item[1] for item in batch]\n",
        "\n",
        "        targets = pad_sequence(targets, batch_first=True, padding_value=self.pad_idx)\n",
        "\n",
        "        return imgs, targets\n",
        "\n",
        "def get_loader(\n",
        "    root_folder: str,\n",
        "    annotation_file: str,\n",
        "    transform: Any,\n",
        "    batch_size: int = 32,\n",
        "    num_workers: int = 8,\n",
        "    shuffle: bool = True,\n",
        "    pin_memory: bool = True,\n",
        ") -> Tuple[DataLoader, FlickrDataset]:\n",
        "\n",
        "    dataset = FlickrDataset(root_folder, annotation_file, transform=transform)\n",
        "    pad_idx = dataset.vocab.stoi[\"<PAD>\"]\n",
        "\n",
        "    loader = DataLoader(\n",
        "        dataset=dataset,\n",
        "        batch_size=batch_size,\n",
        "        num_workers=num_workers,\n",
        "        shuffle=shuffle,\n",
        "        pin_memory=pin_memory,\n",
        "        collate_fn=MyCollate(pad_idx=pad_idx),\n",
        "    )\n",
        "\n",
        "    return loader, dataset"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ba68fca"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import math\n",
        "\n",
        "class EncoderCNN:\n",
        "\n",
        "    def __init__(self, embed_size, train_CNN=False):\n",
        "        super(EncoderCNN, self).__init__()\n",
        "        resnet = models.resnet101(weights=models.ResNet101_Weights.DEFAULT)\n",
        "\n",
        "        modules = list(resnet.children())[:-2]\n",
        "        self.resnet = nn.Sequential(*modules)\n",
        "\n",
        "        self.embed = nn.Linear(2048, embed_size)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.train_CNN = train_CNN\n",
        "\n",
        "    def forward(self, images):\n",
        "        with torch.set_grad_enabled(self.train_CNN):\n",
        "            features = self.resnet(images)\n",
        "\n",
        "\n",
        "        features = features.permute(0, 2, 3, 1)\n",
        "        features = features.view(features.size(0), -1, features.size(3))\n",
        "\n",
        "\n",
        "        features = self.embed(features)\n",
        "        features = self.relu(features)\n",
        "\n",
        "        return features\n",
        "\n",
        "class PositionalEncoding:\n",
        "\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1) # [Max_Len, 1, D_Model]\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:x.size(0), :]\n",
        "\n",
        "class DecoderTransformer:\n",
        "    def __init__(self, embed_size, vocab_size, num_heads=4, num_layers=2, max_len=100):\n",
        "        super(DecoderTransformer, self).__init__()\n",
        "\n",
        "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
        "        self.pos_encoder = PositionalEncoding(embed_size, max_len)\n",
        "\n",
        "\n",
        "        decoder_layer = nn.TransformerDecoderLayer(d_model=embed_size, nhead=num_heads)\n",
        "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
        "\n",
        "        self.linear = nn.Linear(embed_size, vocab_size)\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def forward(self, features, captions):\n",
        "\n",
        "        # Embed captions\n",
        "        embeddings = self.embed(captions) * math.sqrt(features.size(-1))\n",
        "\n",
        "\n",
        "        embeddings = embeddings.permute(1, 0, 2)\n",
        "        features = features.permute(1, 0, 2)\n",
        "\n",
        "        # positional encoding\n",
        "        embeddings = self.pos_encoder(embeddings)\n",
        "\n",
        "        tgt_mask = self.generate_square_subsequent_mask(embeddings.size(0)).to(features.device)\n",
        "\n",
        "        outputs = self.transformer_decoder(tgt=embeddings, memory=features, tgt_mask=tgt_mask)\n",
        "\n",
        "        outputs = self.linear(outputs)\n",
        "\n",
        "        return outputs.permute(1, 0, 2)\n",
        "\n",
        "    def generate_square_subsequent_mask(self, sz):\n",
        "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
        "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "        return mask\n",
        "\n",
        "    def caption_image(self, image_features, vocabulary, max_length=20):\n",
        "\n",
        "        batch_size = image_features.size(0)\n",
        "        start_token = vocabulary.stoi[\"<SOS>\"]\n",
        "\n",
        "        generated = torch.tensor([start_token]).unsqueeze(1).to(image_features.device)\n",
        "\n",
        "        image_features = image_features.permute(1, 0, 2)\n",
        "\n",
        "        result_caption = []\n",
        "\n",
        "        for _ in range(max_length):\n",
        "\n",
        "            tgt_emb = self.embed(generated) * math.sqrt(image_features.size(-1))\n",
        "            tgt_emb = tgt_emb.permute(1, 0, 2)\n",
        "            tgt_emb = self.pos_encoder(tgt_emb)\n",
        "\n",
        "            # Mask\n",
        "            mask = self.generate_square_subsequent_mask(tgt_emb.size(0)).to(image_features.device)\n",
        "\n",
        "            # Decode\n",
        "            out = self.transformer_decoder(tgt=tgt_emb, memory=image_features, tgt_mask=mask)\n",
        "\n",
        "            # Get last token output\n",
        "            last_token_out = out[-1, :, :]\n",
        "\n",
        "            # Predict\n",
        "            logits = self.linear(last_token_out)\n",
        "            predicted_id = logits.argmax(1).item()\n",
        "\n",
        "            result_caption.append(vocabulary.itos[predicted_id])\n",
        "\n",
        "            if vocabulary.itos[predicted_id] == \"<EOS>\":\n",
        "                break\n",
        "\n",
        "\n",
        "            next_token = torch.tensor([predicted_id]).unsqueeze(1).to(image_features.device)\n",
        "            generated = torch.cat((generated, next_token), dim=1)\n",
        "\n",
        "        return result_caption\n",
        "\n",
        "class ImageCaptionModel:\n",
        "    def __init__(self, embed_size, hidden_size, vocab_size, num_heads, num_layers, train_CNN=False):\n",
        "        super(ImageCaptionModel, self).__init__()\n",
        "        self.encoder = EncoderCNN(embed_size, train_CNN)\n",
        "        self.decoder = DecoderTransformer(embed_size, vocab_size, num_heads, num_layers)\n",
        "\n",
        "    def forward(self, images, captions):\n",
        "        features = self.encoder(images)\n",
        "        outputs = self.decoder(features, captions)\n",
        "        return outputs\n",
        "\n",
        "    def caption_image(self, image, vocabulary, max_length=20):\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            features = self.encoder(image.unsqueeze(0))\n",
        "            return self.decoder.caption_image(features, vocabulary, max_length)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f17d132c"
      },
      "source": [
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "\n",
        "def print_examples(model, device, dataset):\n",
        "    \"\"\"\n",
        "    Helper to print predicted captions for a few images during training\n",
        "    to manually verify progress.\n",
        "    \"\"\"\n",
        "    transform = transforms.Compose(\n",
        "        [\n",
        "            transforms.Resize((224, 224)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "\n",
        "    test_img_dir = \"images/\"\n",
        "\n",
        "    import os\n",
        "    try:\n",
        "        test_images = [f for f in os.listdir(test_img_dir) if f.endswith('.jpg')][:2]\n",
        "    except FileNotFoundError:\n",
        "        print(\"Image directory not found, skipping examples.\")\n",
        "        return\n",
        "\n",
        "    for img_name in test_images:\n",
        "        image = Image.open(os.path.join(test_img_dir, img_name)).convert(\"RGB\")\n",
        "        image_tensor = transform(image).unsqueeze(0).to(device)\n",
        "\n",
        "        # Generate caption\n",
        "        with torch.no_grad():\n",
        "            caption = model.caption_image(image_tensor.squeeze(0), dataset.vocab)\n",
        "\n",
        "        print(f\"Image: {img_name}\")\n",
        "        print(f\"Prediction: {' '.join(caption)}\")\n",
        "        print(\"-\" * 20)\n",
        "\n",
        "    model.train()\n",
        "\n",
        "def save_checkpoint(state, filename=\"my_checkpoint.pth.tar\"):\n",
        "    print(\"=> Saving checkpoint\")\n",
        "    torch.save(state, filename)\n",
        "\n",
        "def load_checkpoint(checkpoint, model, optimizer):\n",
        "    print(\"=> Loading checkpoint\")\n",
        "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
        "    optimizer.load_state_dict(checkpoint[\"optimizer\"])"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f508a6ce"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import math\n",
        "\n",
        "class EncoderCNN(nn.Module):\n",
        "\n",
        "    def __init__(self, embed_size, train_CNN=False):\n",
        "        super(EncoderCNN, self).__init__()\n",
        "        resnet = models.resnet101(weights=models.ResNet101_Weights.DEFAULT)\n",
        "\n",
        "        modules = list(resnet.children())[:-2]\n",
        "        self.resnet = nn.Sequential(*modules)\n",
        "\n",
        "        self.embed = nn.Linear(2048, embed_size)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.train_CNN = train_CNN\n",
        "\n",
        "    def forward(self, images):\n",
        "        with torch.set_grad_enabled(self.train_CNN):\n",
        "            features = self.resnet(images)\n",
        "\n",
        "\n",
        "        features = features.permute(0, 2, 3, 1)\n",
        "        features = features.view(features.size(0), -1, features.size(3))\n",
        "\n",
        "\n",
        "        features = self.embed(features)\n",
        "        features = self.relu(features)\n",
        "\n",
        "        return features\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1) # [Max_Len, 1, D_Model]\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:x.size(0), :]\n",
        "\n",
        "class DecoderTransformer(nn.Module):\n",
        "    def __init__(self, embed_size, vocab_size, num_heads=4, num_layers=2, max_len=100):\n",
        "        super(DecoderTransformer, self).__init__()\n",
        "\n",
        "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
        "        self.pos_encoder = PositionalEncoding(embed_size, max_len)\n",
        "\n",
        "\n",
        "        decoder_layer = nn.TransformerDecoderLayer(d_model=embed_size, nhead=num_heads)\n",
        "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
        "\n",
        "        self.linear = nn.Linear(embed_size, vocab_size)\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def forward(self, features, captions):\n",
        "\n",
        "        # Embed captions\n",
        "        embeddings = self.embed(captions) * math.sqrt(features.size(-1))\n",
        "\n",
        "\n",
        "        embeddings = embeddings.permute(1, 0, 2)\n",
        "        features = features.permute(1, 0, 2)\n",
        "\n",
        "        # positional encoding\n",
        "        embeddings = self.pos_encoder(embeddings)\n",
        "\n",
        "        tgt_mask = self.generate_square_subsequent_mask(embeddings.size(0)).to(features.device)\n",
        "\n",
        "        outputs = self.transformer_decoder(tgt=embeddings, memory=features, tgt_mask=tgt_mask)\n",
        "\n",
        "        outputs = self.linear(outputs)\n",
        "\n",
        "        return outputs.permute(1, 0, 2)\n",
        "\n",
        "    def generate_square_subsequent_mask(self, sz):\n",
        "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
        "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "        return mask\n",
        "\n",
        "    def caption_image(self, image_features, vocabulary, max_length=20):\n",
        "\n",
        "        batch_size = image_features.size(0)\n",
        "        start_token = vocabulary.stoi[\"<SOS>\"]\n",
        "\n",
        "        generated = torch.tensor([start_token]).unsqueeze(1).to(image_features.device)\n",
        "\n",
        "        image_features = image_features.permute(1, 0, 2)\n",
        "\n",
        "        result_caption = []\n",
        "\n",
        "        for _ in range(max_length):\n",
        "\n",
        "            tgt_emb = self.embed(generated) * math.sqrt(image_features.size(-1))\n",
        "            tgt_emb = tgt_emb.permute(1, 0, 2)\n",
        "            tgt_emb = self.pos_encoder(tgt_emb)\n",
        "\n",
        "            # Mask\n",
        "            mask = self.generate_square_subsequent_mask(tgt_emb.size(0)).to(image_features.device)\n",
        "\n",
        "            # Decode\n",
        "            out = self.transformer_decoder(tgt=tgt_emb, memory=image_features, tgt_mask=mask)\n",
        "\n",
        "            # Get last token output\n",
        "            last_token_out = out[-1, :, :]\n",
        "\n",
        "            # Predict\n",
        "            logits = self.linear(last_token_out)\n",
        "            predicted_id = logits.argmax(1).item()\n",
        "\n",
        "            result_caption.append(vocabulary.itos[predicted_id])\n",
        "\n",
        "            if vocabulary.itos[predicted_id] == \"<EOS>\":\n",
        "                break\n",
        "\n",
        "\n",
        "            next_token = torch.tensor([predicted_id]).unsqueeze(1).to(image_features.device)\n",
        "            generated = torch.cat((generated, next_token), dim=1)\n",
        "\n",
        "        return result_caption\n",
        "\n",
        "class ImageCaptionModel(nn.Module):\n",
        "    def __init__(self, embed_size, hidden_size, vocab_size, num_heads, num_layers, train_CNN=False):\n",
        "        super(ImageCaptionModel, self).__init__()\n",
        "        self.encoder = EncoderCNN(embed_size, train_CNN)\n",
        "        self.decoder = DecoderTransformer(embed_size, vocab_size, num_heads, num_layers)\n",
        "\n",
        "    def forward(self, images, captions):\n",
        "        features = self.encoder(images)\n",
        "        outputs = self.decoder(features, captions)\n",
        "        return outputs\n",
        "\n",
        "    def caption_image(self, image, vocabulary, max_length=20):\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            features = self.encoder(image.unsqueeze(0))\n",
        "            return self.decoder.caption_image(features, vocabulary, max_length)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "22b6c910",
        "outputId": "442abfcf-1006-47fb-9807-1e416e8b6c5d"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "# from data_loader import get_loader\n",
        "# from model import ImageCaptionModel\n",
        "# from utils import save_checkpoint, load_checkpoint, print_examples\n",
        "\n",
        "#  Hyperparameters\n",
        "embed_size = 256\n",
        "hidden_size = 256\n",
        "vocab_size = -1\n",
        "num_heads = 4\n",
        "num_layers = 2\n",
        "learning_rate = 3e-4\n",
        "num_epochs = 100\n",
        "batch_size = 32\n",
        "num_workers = 2\n",
        "load_model = False\n",
        "save_model = True\n",
        "train_CNN = False\n",
        "def train():\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "\n",
        "    transform = transforms.Compose(\n",
        "        [\n",
        "            transforms.Resize((232, 232)),\n",
        "            transforms.RandomCrop((224, 224)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "\n",
        "    print(\"Loading Data...\")\n",
        "    loader, dataset = get_loader(\n",
        "        root_folder=\"images/\",\n",
        "        annotation_file=\"captions.txt\",\n",
        "        transform=transform,\n",
        "        batch_size=batch_size,\n",
        "        num_workers=num_workers,\n",
        "    )\n",
        "\n",
        "    vocab_size = len(dataset.vocab)\n",
        "    print(f\"Vocabulary Size: {vocab_size}\")\n",
        "\n",
        "    model = ImageCaptionModel(\n",
        "        embed_size=embed_size,\n",
        "        hidden_size=hidden_size,\n",
        "        vocab_size=vocab_size,\n",
        "        num_heads=num_heads,\n",
        "        num_layers=num_layers,\n",
        "        train_CNN=train_CNN\n",
        "    ).to(device)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=dataset.vocab.stoi[\"<PAD>\"])\n",
        "\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    if load_model:\n",
        "        load_checkpoint(torch.load(\"my_checkpoint.pth.tar\"), model, optimizer)\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"\\n--- Epoch {epoch+1}/{num_epochs} ---\")\n",
        "\n",
        "        if save_model and epoch % 5 == 0:\n",
        "            checkpoint = {\n",
        "                \"state_dict\": model.state_dict(),\n",
        "                \"optimizer\": optimizer.state_dict(),\n",
        "            }\n",
        "            save_checkpoint(checkpoint)\n",
        "\n",
        "\n",
        "\n",
        "        loop = tqdm(loader, leave=True)\n",
        "\n",
        "        for idx, (imgs, captions) in enumerate(loop):\n",
        "            imgs = imgs.to(device)\n",
        "            captions = captions.to(device)\n",
        "\n",
        "\n",
        "            outputs = model(imgs, captions[:, :-1])\n",
        "\n",
        "\n",
        "            loss = criterion(\n",
        "                outputs.reshape(-1, outputs.shape[2]),\n",
        "                captions[:, 1:].reshape(-1)\n",
        "            )\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "\n",
        "\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            loop.set_description(f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
        "            loop.set_postfix(loss=loss.item())\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Loading Data...\n",
            "Vocabulary Size: 12\n",
            "\n",
            "--- Epoch 1/100 ---\n",
            "=> Saving checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [1/100]: 100%|██████████| 1/1 [00:00<00:00,  1.96it/s, loss=2.34]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch 2/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [2/100]: 100%|██████████| 1/1 [00:00<00:00,  3.59it/s, loss=1.27]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch 3/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [3/100]: 100%|██████████| 1/1 [00:00<00:00,  3.66it/s, loss=0.683]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch 4/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [4/100]: 100%|██████████| 1/1 [00:00<00:00,  3.61it/s, loss=0.375]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch 5/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [5/100]: 100%|██████████| 1/1 [00:00<00:00,  3.70it/s, loss=0.224]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch 6/100 ---\n",
            "=> Saving checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [6/100]: 100%|██████████| 1/1 [00:00<00:00,  3.31it/s, loss=0.169]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch 7/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [7/100]: 100%|██████████| 1/1 [00:00<00:00,  3.57it/s, loss=0.143]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch 8/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [8/100]: 100%|██████████| 1/1 [00:00<00:00,  3.69it/s, loss=0.138]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch 9/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [9/100]: 100%|██████████| 1/1 [00:00<00:00,  3.54it/s, loss=0.127]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch 10/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [10/100]: 100%|██████████| 1/1 [00:00<00:00,  3.49it/s, loss=0.129]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch 11/100 ---\n",
            "=> Saving checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [11/100]: 100%|██████████| 1/1 [00:00<00:00,  3.54it/s, loss=0.128]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch 12/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [12/100]: 100%|██████████| 1/1 [00:00<00:00,  3.52it/s, loss=0.134]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch 13/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [13/100]: 100%|██████████| 1/1 [00:00<00:00,  3.74it/s, loss=0.122]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch 14/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [14/100]: 100%|██████████| 1/1 [00:00<00:00,  3.59it/s, loss=0.14]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch 15/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [15/100]: 100%|██████████| 1/1 [00:00<00:00,  3.69it/s, loss=0.117]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch 16/100 ---\n",
            "=> Saving checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [16/100]: 100%|██████████| 1/1 [00:00<00:00,  2.58it/s, loss=0.141]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch 17/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [17/100]: 100%|██████████| 1/1 [00:00<00:00,  2.82it/s, loss=0.114]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch 18/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [18/100]: 100%|██████████| 1/1 [00:00<00:00,  2.64it/s, loss=0.11]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch 19/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [19/100]: 100%|██████████| 1/1 [00:00<00:00,  3.03it/s, loss=0.122]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch 20/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [20/100]: 100%|██████████| 1/1 [00:00<00:00,  3.68it/s, loss=0.138]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch 21/100 ---\n",
            "=> Saving checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [21/100]: 100%|██████████| 1/1 [00:00<00:00,  3.65it/s, loss=0.106]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch 22/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [22/100]: 100%|██████████| 1/1 [00:00<00:00,  3.66it/s, loss=0.127]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch 23/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [23/100]: 100%|██████████| 1/1 [00:00<00:00,  3.70it/s, loss=0.116]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch 24/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [24/100]: 100%|██████████| 1/1 [00:00<00:00,  3.61it/s, loss=0.119]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch 25/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [25/100]: 100%|██████████| 1/1 [00:00<00:00,  3.72it/s, loss=0.11]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch 26/100 ---\n",
            "=> Saving checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [26/100]: 100%|██████████| 1/1 [00:00<00:00,  3.48it/s, loss=0.132]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch 27/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [27/100]: 100%|██████████| 1/1 [00:00<00:00,  3.54it/s, loss=0.112]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch 28/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [28/100]: 100%|██████████| 1/1 [00:00<00:00,  3.42it/s, loss=0.131]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch 29/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [29/100]: 100%|██████████| 1/1 [00:00<00:00,  3.76it/s, loss=0.129]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch 30/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [30/100]: 100%|██████████| 1/1 [00:00<00:00,  3.78it/s, loss=0.107]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch 31/100 ---\n",
            "=> Saving checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [31/100]: 100%|██████████| 1/1 [00:00<00:00,  3.52it/s, loss=0.138]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch 32/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [32/100]: 100%|██████████| 1/1 [00:00<00:00,  3.74it/s, loss=0.124]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch 33/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [33/100]: 100%|██████████| 1/1 [00:00<00:00,  3.70it/s, loss=0.121]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch 34/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [34/100]: 100%|██████████| 1/1 [00:00<00:00,  3.33it/s, loss=0.123]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch 35/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [35/100]: 100%|██████████| 1/1 [00:00<00:00,  3.78it/s, loss=0.126]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch 36/100 ---\n",
            "=> Saving checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [36/100]: 100%|██████████| 1/1 [00:00<00:00,  3.55it/s, loss=0.111]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch 37/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [37/100]: 100%|██████████| 1/1 [00:00<00:00,  3.45it/s, loss=0.115]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch 38/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [38/100]: 100%|██████████| 1/1 [00:00<00:00,  3.65it/s, loss=0.12]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch 39/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [39/100]: 100%|██████████| 1/1 [00:00<00:00,  3.61it/s, loss=0.118]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch 40/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [40/100]: 100%|██████████| 1/1 [00:00<00:00,  3.07it/s, loss=0.135]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch 41/100 ---\n",
            "=> Saving checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [41/100]: 100%|██████████| 1/1 [00:00<00:00,  3.32it/s, loss=0.133]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch 42/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [42/100]: 100%|██████████| 1/1 [00:00<00:00,  3.33it/s, loss=0.119]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch 43/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [43/100]: 100%|██████████| 1/1 [00:00<00:00,  3.65it/s, loss=0.123]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch 44/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [44/100]: 100%|██████████| 1/1 [00:00<00:00,  3.64it/s, loss=0.117]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch 45/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [45/100]: 100%|██████████| 1/1 [00:00<00:00,  3.56it/s, loss=0.133]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch 46/100 ---\n",
            "=> Saving checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [46/100]: 100%|██████████| 1/1 [00:00<00:00,  3.51it/s, loss=0.113]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch 47/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [47/100]: 100%|██████████| 1/1 [00:00<00:00,  3.44it/s, loss=0.132]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch 48/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [48/100]: 100%|██████████| 1/1 [00:00<00:00,  3.74it/s, loss=0.11]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch 49/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [49/100]: 100%|██████████| 1/1 [00:00<00:00,  3.73it/s, loss=0.116]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch 50/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [50/100]: 100%|██████████| 1/1 [00:00<00:00,  3.71it/s, loss=0.122]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch 51/100 ---\n",
            "=> Saving checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [51/100]: 100%|██████████| 1/1 [00:00<00:00,  3.42it/s, loss=0.113]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch 52/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [52/100]: 100%|██████████| 1/1 [00:00<00:00,  3.68it/s, loss=0.139]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch 53/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [53/100]: 100%|██████████| 1/1 [00:00<00:00,  3.34it/s, loss=0.118]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch 54/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [54/100]: 100%|██████████| 1/1 [00:00<00:00,  3.72it/s, loss=0.108]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch 55/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [55/100]: 100%|██████████| 1/1 [00:00<00:00,  3.66it/s, loss=0.131]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch 56/100 ---\n",
            "=> Saving checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [56/100]: 100%|██████████| 1/1 [00:00<00:00,  3.43it/s, loss=0.114]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch 57/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [57/100]: 100%|██████████| 1/1 [00:00<00:00,  3.63it/s, loss=0.117]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch 58/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [58/100]: 100%|██████████| 1/1 [00:00<00:00,  3.66it/s, loss=0.113]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch 59/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [59/100]: 100%|██████████| 1/1 [00:00<00:00,  3.58it/s, loss=0.122]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch 60/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [60/100]: 100%|██████████| 1/1 [00:00<00:00,  3.66it/s, loss=0.116]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch 61/100 ---\n",
            "=> Saving checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [61/100]: 100%|██████████| 1/1 [00:00<00:00,  3.44it/s, loss=0.123]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch 62/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [62/100]: 100%|██████████| 1/1 [00:00<00:00,  3.47it/s, loss=0.126]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch 63/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [63/100]: 100%|██████████| 1/1 [00:00<00:00,  3.59it/s, loss=0.124]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch 64/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [64/100]: 100%|██████████| 1/1 [00:00<00:00,  3.55it/s, loss=0.132]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch 65/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [65/100]: 100%|██████████| 1/1 [00:00<00:00,  3.68it/s, loss=0.122]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch 66/100 ---\n",
            "=> Saving checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [66/100]: 100%|██████████| 1/1 [00:00<00:00,  3.31it/s, loss=0.125]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch 67/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [67/100]: 100%|██████████| 1/1 [00:00<00:00,  3.54it/s, loss=0.128]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch 68/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [68/100]: 100%|██████████| 1/1 [00:00<00:00,  3.60it/s, loss=0.128]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch 69/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [69/100]: 100%|██████████| 1/1 [00:00<00:00,  3.70it/s, loss=0.119]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch 70/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [70/100]: 100%|██████████| 1/1 [00:00<00:00,  3.52it/s, loss=0.105]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch 71/100 ---\n",
            "=> Saving checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [71/100]: 100%|██████████| 1/1 [00:00<00:00,  3.41it/s, loss=0.115]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch 72/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [72/100]: 100%|██████████| 1/1 [00:00<00:00,  3.62it/s, loss=0.115]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch 73/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [73/100]: 100%|██████████| 1/1 [00:00<00:00,  3.68it/s, loss=0.107]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch 74/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [74/100]: 100%|██████████| 1/1 [00:00<00:00,  3.54it/s, loss=0.114]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch 75/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [75/100]: 100%|██████████| 1/1 [00:00<00:00,  3.73it/s, loss=0.123]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch 76/100 ---\n",
            "=> Saving checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [76/100]: 100%|██████████| 1/1 [00:00<00:00,  3.39it/s, loss=0.12]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch 77/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [77/100]: 100%|██████████| 1/1 [00:00<00:00,  3.55it/s, loss=0.116]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch 78/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [78/100]: 100%|██████████| 1/1 [00:00<00:00,  3.64it/s, loss=0.117]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch 79/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [79/100]: 100%|██████████| 1/1 [00:00<00:00,  3.64it/s, loss=0.114]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch 80/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [80/100]: 100%|██████████| 1/1 [00:00<00:00,  3.53it/s, loss=0.115]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch 81/100 ---\n",
            "=> Saving checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [81/100]: 100%|██████████| 1/1 [00:00<00:00,  3.43it/s, loss=0.124]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch 82/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [82/100]: 100%|██████████| 1/1 [00:00<00:00,  3.53it/s, loss=0.126]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch 83/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [83/100]: 100%|██████████| 1/1 [00:00<00:00,  3.67it/s, loss=0.135]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch 84/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [84/100]: 100%|██████████| 1/1 [00:00<00:00,  3.66it/s, loss=0.106]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch 85/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [85/100]: 100%|██████████| 1/1 [00:00<00:00,  3.60it/s, loss=0.121]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch 86/100 ---\n",
            "=> Saving checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [86/100]: 100%|██████████| 1/1 [00:00<00:00,  3.40it/s, loss=0.117]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch 87/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [87/100]: 100%|██████████| 1/1 [00:00<00:00,  3.59it/s, loss=0.116]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch 88/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [88/100]: 100%|██████████| 1/1 [00:00<00:00,  3.58it/s, loss=0.12]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch 89/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [89/100]: 100%|██████████| 1/1 [00:00<00:00,  3.54it/s, loss=0.119]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch 90/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [90/100]: 100%|██████████| 1/1 [00:00<00:00,  3.75it/s, loss=0.115]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch 91/100 ---\n",
            "=> Saving checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [91/100]: 100%|██████████| 1/1 [00:00<00:00,  3.10it/s, loss=0.113]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch 92/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [92/100]: 100%|██████████| 1/1 [00:00<00:00,  3.62it/s, loss=0.114]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch 93/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [93/100]: 100%|██████████| 1/1 [00:00<00:00,  3.65it/s, loss=0.129]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch 94/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [94/100]: 100%|██████████| 1/1 [00:00<00:00,  3.18it/s, loss=0.112]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch 95/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [95/100]: 100%|██████████| 1/1 [00:00<00:00,  3.64it/s, loss=0.111]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch 96/100 ---\n",
            "=> Saving checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [96/100]: 100%|██████████| 1/1 [00:00<00:00,  3.37it/s, loss=0.117]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch 97/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [97/100]: 100%|██████████| 1/1 [00:00<00:00,  3.54it/s, loss=0.127]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch 98/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [98/100]: 100%|██████████| 1/1 [00:00<00:00,  3.59it/s, loss=0.121]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch 99/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [99/100]: 100%|██████████| 1/1 [00:00<00:00,  3.63it/s, loss=0.12]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch 100/100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [100/100]: 100%|██████████| 1/1 [00:00<00:00,  2.80it/s, loss=0.113]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "torch.save(trained_model.state_dict(), \"my_best_model.pth\")\n",
        "print(\"Model saved to disk!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s6_3M1rZ_bDB",
        "outputId": "4fcd412d-d99a-49b7-c96b-0194b65e575f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved to disk!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "trained_model = ImageCaptionModel(embed_size, vocab_size, num_heads, num_layers).to(device)\n",
        "\n",
        "trained_model.load_state_dict(torch.load(\"my_best_model.pth\"))\n",
        "print(\"Model loaded!\")\n"
      ],
      "metadata": {
        "id": "VCn26ttVAlUE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#deployment"
      ],
      "metadata": {
        "id": "z1sKCN0hAhLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Installing deployment tools...\")\n",
        "!pip install -q fastapi uvicorn pyngrok python-multipart nest-asyncio prometheus-fastapi-instrumentator\n",
        "\n",
        "import uvicorn\n",
        "import nest_asyncio\n",
        "from fastapi import FastAPI, UploadFile, File\n",
        "from fastapi.responses import JSONResponse\n",
        "from prometheus_fastapi_instrumentator import Instrumentator\n",
        "from pyngrok import ngrok\n",
        "from PIL import Image\n",
        "import io\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Import necessary components from previously defined cells\n",
        "# (Assuming get_loader, Vocabulary, ImageCaptionModel are defined in prior executed cells)\n",
        "# To make dataset globally available for prediction\n",
        "\n",
        "# Initialize dataset and vocab (assuming get_loader is available from an executed cell)\n",
        "# Re-using transform definition from training for consistency\n",
        "transform_deploy = transforms.Compose(\n",
        "    [\n",
        "        transforms.Resize((232, 232)),\n",
        "        transforms.RandomCrop((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "    ]\n",
        ")\n",
        "_, dataset = get_loader(\n",
        "    root_folder=\"images/\", # Make sure 'images/' directory and 'captions.txt' are correctly set up\n",
        "    annotation_file=\"captions.txt\",\n",
        "    transform=transform_deploy,\n",
        "    batch_size=1, # Batch size 1 for single image prediction\n",
        "    shuffle=False,\n",
        "    num_workers=0 # No workers needed for single image prediction\n",
        ")\n",
        "\n",
        "app = FastAPI(\n",
        "    title=\"Image Captioning API\",\n",
        "    description=\"Karpathy-style Image Captioning deployed from Colab\",\n",
        "    version=\"1.0\"\n",
        ")\n",
        "\n",
        "\n",
        "Instrumentator().instrument(app).expose(app)\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "deploy_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "@app.get(\"/\")\n",
        "def home():\n",
        "    return {\"status\": \"online\", \"model\": \"Transformer-ResNet101\"}\n",
        "\n",
        "@app.post(\"/predict\")\n",
        "async def predict(file: UploadFile = File(...)):\n",
        "\n",
        "    try:\n",
        "\n",
        "        contents = await file.read()\n",
        "        image = Image.open(io.BytesIO(contents)).convert(\"RGB\")\n",
        "\n",
        "\n",
        "        img_tensor = deploy_transform(image).unsqueeze(0).to(device)\n",
        "\n",
        "\n",
        "        caption_list = trained_model.caption_image(img_tensor.squeeze(0), dataset.vocab)\n",
        "\n",
        "\n",
        "        caption_text = \" \".join(caption_list)\n",
        "\n",
        "\n",
        "        caption_text = caption_text.replace(\"<SOS>\", \"\").replace(\"<EOS>\", \"\").strip()\n",
        "\n",
        "        return {\n",
        "            \"filename\": file.filename,\n",
        "            \"caption\": caption_text\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        return JSONResponse(status_code=500, content={\"error\": str(e)})\n",
        "\n",
        "\n",
        "NGROK_AUTH_TOKEN = \"37nEPCc9Sa1UOCH5a6tA91p7e8H_2V6u4Cje5VLoHZyejxGWf\"\n",
        "if NGROK_AUTH_TOKEN != \"37nEPCc9Sa1UOCH5a6tA91p7e8H_2V6u4Cje5VLoHZyejxGWf\":\n",
        "    ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
        "\n",
        "\n",
        "    public_url = ngrok.connect(8000).public_url\n",
        "    print(f\"\\n API IS LIVE! Public URL: {public_url}\")\n",
        "    print(f\" Monitoring Metrics: {public_url}/metrics\")\n",
        "    print(f\" Interactive Docs: {public_url}/docs\")\n",
        "\n",
        "\n",
        "    nest_asyncio.apply()\n",
        "    uvicorn.run(app, port=8000)\n",
        "else:\n",
        "    print(\"⚠️ Please paste your ngrok token in the code above to generate a public URL.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S1B-o8L_A0OM",
        "outputId": "bdbeb79b-cae0-48fd-a8a1-ff10fcce8843"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing deployment tools...\n",
            "⚠️ Please paste your ngrok token in the code above to generate a public URL.\n"
          ]
        }
      ]
    }
  ]
}